---
title: "GBSG2 Exploration"
author: "Sep Dadsetan"
date: "4/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(skimr)
library(naniar)
library(GGally)
library(rpart) # recursive partitioning (decision tree)
library(caret)
library(knitr)
library(kableExtra)
library(survival)
library(randomForest)
```


### Description

The German Breast Cancer Study Group 2 (GBSG2) is a dataset consisting of 686 female breast cancer observations with 10 features (see below). Goal of this document will be to characterize the dataset and apply verious machine learning algorithms to help us predict the length of time which future patients may reach a censoring event.

#### Feature Descriptions

- **horTh**: hormonal therapy, a factor at two levels no and yes.
- **age**: of the patients in years.
- **menostat**: menopausal status, a factor at two levels `pre` (premenopausal) and `post` (postmenopausal).
- **tsize**: tumor size (in mm).
- **tgrade**: tumor grade, a ordered factor at levels `I < II < III`.
- **pnodes**: number of positive nodes.
- **progrec**: progesterone receptor (in fmol).
- **estrec**: estrogen receptor (in fmol).
- **time**: recurrence free survival time (in days).
- **cens**: censoring indicator (0- censored, 1- event)

### Load Data

```{r}
data("GBSG2", package = "TH.data")
gbsg2 <- GBSG2 %>% as_tibble() %>% clean_names()
```

### Explore Characteristics

Let's take a look at characteristics like dimension, feature, missingness, distribution, etc. As we stated above there are 686 observations x 10 features. `hor_th` and `menostat` are factors, `tgrade` is an ordinal, and the rest are integers. 

```{r head tibble}
# Print head of tibble so we can look at a few values
head(gbsg2) %>% 
  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"))
```

A quick summary of the features shows us a few points of interest:

- there are no missing values (expected from an example dataset)
- mean age of the dataset if 53 yo
- mean recurrence free survival of about 3 years

```{r skim and missingness}
# Use skim to look at summaries and missingness
skim(gbsg2)
```

### Manipulate Data

While we're at it, I think years is a bit easier to understand than days so let's add that column.

```{r mutate columns}
gbsg2 <- gbsg2 %>% 
  mutate(time_yrs = time/365, cens_fct = as.factor(cens))
```


```{r ggpairs, eval=F, echo=F}
### Visualize broadly
# Let's use ggpairs to look at a few interesting cross-sections of features colored by censor group.
ggpairs(gbsg2, mapping = aes(color = cens_fct), columns = c("hor_th", "age", "time_yrs", "tgrade"))
```

### Survival Plot by Tumor Grade

As expected, we see that patients with lower tumor grades have longer lengths of recurrence free survival.
```{r survival plot, fig.align='center'}
gbsg_surv <- survival::survfit(Surv(time_yrs, cens) ~ tgrade, data = gbsg2)
ggsurv(gbsg_surv, 
       main = "Survival Plot of Female Breast Cancer Patients by Tumor Grade",
       xlab = "Time (yrs)",
       ylab = "Survival (percent)")
```


### Build Models 

Let's explore different models from linear discriminate analysis to random forest and see which one provides us the most accurate prediction.

TODO - fill in details of each algorithm and its pros/cons

- Linear Discriminant Analysis (LDA)
- Classification and Regression Trees (CART)
- k-Nearest Neighbor (kNN)
- Support Vector Machine (SVM)
- Random Forest (RF)

```{r warning=F}
# Break up sample into train and test
# set.seed(123)
# assignment <- sample(1:2, nrow(gbsg2), prob = c(0.7, 0.3), replace = T)

# Alternative method of sampling
train_index <- createDataPartition(gbsg2$cens, p = 0.8, list = F)
gbsg2_train <- gbsg2[train_index,] %>% select(-cens)
gbsg2_test <- gbsg2[-train_index,] %>% select(-cens)

# add groups to gbsg2 tibble
# gbsg2 <- gbsg2 %>% mutate(group = assignment)
# gbsg2_train <- gbsg2 %>% filter(group == 1)
# gbsg2_test <- gbsg2 %>% filter(group == 2)

# Setup 10-fold cross validation
control <- trainControl(method = "cv", number = 10)
metric <- "Accuracy"

# a) linear algorithms
set.seed(7)
fit.lda <- train(cens_fct ~ ., data = gbsg2_train, method = "lda", metric = metric, trControl = control)

# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(cens_fct ~ ., data = gbsg2_train, method = "rpart", metric = metric, trControl = control)
# kNN
set.seed(7)
fit.knn <- train(cens_fct ~ ., data = gbsg2_train, method = "knn", metric = metric, trControl = control)

# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- train(cens_fct ~ ., data = gbsg2_train, method = "svmRadial", metric = metric, trControl = control)
# Random Forest
set.seed(7)
fit.rf <- train(cens_fct ~ ., data = gbsg2_train, method = "rf", metric = metric, trControl = control)

# summarize accuracy of models
results <- resamples(list(lda = fit.lda, cart = fit.cart, knn = fit.knn, svm = fit.svm, rf = fit.rf))
summary(results)

dotplot(results)

print(fit.rf)

# estimate skill of RF on the validation dataset
predictions <- predict(fit.rf, gbsg2_test)
confusionMatrix(predictions, gbsg2_test$cens_fct)
```